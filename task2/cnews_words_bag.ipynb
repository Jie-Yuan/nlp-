{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnews文本数据矩阵化成词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、先读取数据并查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            content\n",
      "0    体育  马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...\n",
      "1    体育  商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...\n",
      "2    体育  冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...\n",
      "3    体育  辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...\n",
      "4    体育  揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../dataset/cnews/cnews.train.txt',sep='\\t',engine='python',encoding='UTF-8',names=['label','content'])\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、看一下停用词读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['，', '。', '/', ' ', ' ', '！', '■', '#', '{', '}', '（', '）', '[', ']', '【', '】', '“', '”', '+', '-', '_', '(', ')', '~', '`', '*', '－', '—', '～', '•', '●', '⊙', '=', '!', '□', '／', '@', '：', '？', '?', '￥', '$', '、', '‘', '’', \"'\", ';', ',', '.', ':', ';', '\"', '；', '的', '了', '呢', '么', '啊', '们', '你', '我', '它', '他', '她', '你们', '我们', '他们', '她们', '它们', '这', '那', '能', '可', '也', '来', '让', '又', '下', '上', '中', '在', '《', '》', '…', '·', '吧', '吗']\n"
     ]
    }
   ],
   "source": [
    "# 加载停用词列表\n",
    "def get_stopwords(path):\n",
    "    stopwords = open(path,encoding='utf8').read().strip().split('\\n')\n",
    "    return stopwords\n",
    "\n",
    "stopwords = get_stopwords('./stopwords.txt')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本去除停用词\n",
    "def move_stopwords(text, stopwords):\n",
    "    return [i for i in text if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一篇文档转成分词列表形式(并去除停用词)\n",
    "def doc2wordlist(text):\n",
    "    words = jieba.lcut(text)\n",
    "    stopwords = get_stopwords('./stopwords.txt')\n",
    "    return move_stopwords(words,stopwords)\n",
    "\n",
    "# tf = train_data.content.apply(doc2wordlist)\n",
    "# tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# 构建词典，默认取频数最大的前5000词\n",
    "def build_dict(texts,path,max_size=5000):\n",
    "    alldata = []\n",
    "    for i in texts:\n",
    "        alldata.extend(i)\n",
    "    \n",
    "    counter = Counter(alldata)\n",
    "    \n",
    "    wordslist = []\n",
    "    for item in counter.most_common(max_size):\n",
    "        wordslist.append(item[0])\n",
    "\n",
    "    with open(path,'w',encoding='utf-8') as f:\n",
    "        s = '\\n'.join(wordslist)\n",
    "        f.write(s)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取字典，{词语：序号}\n",
    "def get_dict(path):\n",
    "    vocab = {}\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for i,word in enumerate(f):\n",
    "            vocab[word.strip()] = i\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一篇文档词列表 转换 成词袋向量\n",
    "def text2bag(text,vocab):\n",
    "    vec = [0] * len(vocab)\n",
    "\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec[vocab[word]] += 1\n",
    "        except (KeyError,ValueError):\n",
    "            continue\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理，将所有文档的内容处理成词列表，并去除停用词\n",
    "words_list = []\n",
    "for text in train_data.content:\n",
    "    words_list.append(doc2wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是\n",
      "和\n",
      "有\n",
      "都\n",
      "\n",
      "将\n",
      "月\n",
      "就\n",
      "对\n",
      "年\n",
      "为\n",
      "一个\n"
     ]
    }
   ],
   "source": [
    "# 建立字典\n",
    "build_dict(words_list,'./vocab.txt')\n",
    "# 获取词典{单词：序号}\n",
    "vocab = get_dict('./vocab.txt')\n",
    "\n",
    "for i,item in enumerate(vocab):\n",
    "    print(item)\n",
    "    if i>10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 5000)\n",
      "[[2 0 2 ... 0 0 0]\n",
      " [4 3 7 ... 0 0 0]\n",
      " [2 4 2 ... 0 0 0]\n",
      " ...\n",
      " [0 2 3 ... 0 0 0]\n",
      " [0 1 2 ... 0 0 0]\n",
      " [3 7 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 构建词袋X\n",
    "bow = []\n",
    "for i in words_list:\n",
    "    bow.append(text2bag(i,vocab))\n",
    "\n",
    "bow = np.array(bow)\n",
    "print(bow.shape,bow,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用sklearn包工具获取词袋，词频，TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 331413)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text as ft\n",
    "\n",
    "# 构建计数适量化器\n",
    "cv = ft.CountVectorizer()\n",
    "# 获取词袋矩阵\n",
    "bow = cv.fit_transform([' '.join(x) for x in words_list]) #words_list 是已经分词且去停用词的词列表\n",
    "# print(bow)\n",
    "print(bow.shape,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 331413)\n"
     ]
    }
   ],
   "source": [
    "# 词频 TF\n",
    "from sklearn import preprocessing\n",
    "tf = preprocessing.normalize(bow,norm='l1')\n",
    "# print(tf)\n",
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 331413)\n"
     ]
    }
   ],
   "source": [
    "# 构建词频逆文档频率转换器\n",
    "tfidf_model = ft.TfidfTransformer()\n",
    "# TF-IDF\n",
    "tfidf = tfidf_model.fit_transform(bow)\n",
    "# print(tfidf)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
